{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "* 4.5k real voices from FoR\n",
    "* 4.5k fake voices from against Faird + RawNet\n",
    "* 1.0k fake voices from against Deep4SNet (as testing later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files:\n",
      "RQ1/for-real-validation: 5400\n",
      "RQ3/for-bh-madefake-final-r4k: 8720\n",
      "RQ3/for-rawnet-madefake-final-r4k: 1505\n",
      "RQ3/for-deep4s-madefake-final-r4k: 9015\n",
      "\n",
      "Sample sizes:\n",
      "RQ1/for-real-validation: 4500\n",
      "RQ3/for-bh-madefake-final-r4k: 2995\n",
      "RQ3/for-rawnet-madefake-final-r4k: 1505\n",
      "RQ3/for-deep4s-madefake-final-r4k: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Define the directories\n",
    "dir_real_for = r'Data\\DeepVC-Dataset\\RQ1\\for-real-validation'\n",
    "dir_fake_farid = r'Data\\DeepVC-Dataset\\RQ3\\for-bh-madefake-final-r4k'\n",
    "dir_fake_rawnet = r'Data\\DeepVC-Dataset\\RQ3\\for-rawnet-madefake-final-r4k'\n",
    "dir_fake_deep4s = r'Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k'\n",
    "\n",
    "# Function to get .wav files from a directory\n",
    "def get_wav_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.wav')]\n",
    "\n",
    "# Get a list of all .wav file paths in the directories\n",
    "files_real_for = get_wav_files(dir_real_for)\n",
    "files_fake_farid = get_wav_files(dir_fake_farid)\n",
    "files_fake_rawnet = get_wav_files(dir_fake_rawnet)\n",
    "files_fake_deep4s = get_wav_files(dir_fake_deep4s)\n",
    "\n",
    "# Check files exist\n",
    "print(\"All files:\")\n",
    "print(\"RQ1/for-real-validation:\", len(files_real_for))\n",
    "print(\"RQ3/for-bh-madefake-final-r4k:\", len(files_fake_farid))\n",
    "print(\"RQ3/for-rawnet-madefake-final-r4k:\", len(files_fake_rawnet))\n",
    "print(\"RQ3/for-deep4s-madefake-final-r4k:\", len(files_fake_deep4s))\n",
    "\n",
    "# Function to sample files safely\n",
    "def sample_files(file_list, sample_size):\n",
    "    return random.sample(file_list, min(sample_size, len(file_list))) if len(file_list) >= sample_size else file_list\n",
    "\n",
    "# Randomly sample from each file path\n",
    "sample_real = sample_files(files_real_for, 4500)\n",
    "sample_fake_1 = sample_files(files_fake_farid, 2995)\n",
    "sample_fake_2 = sample_files(files_fake_rawnet, 1505)\n",
    "sample_fake_deep4s = sample_files(files_fake_deep4s, 1000)\n",
    "\n",
    "# Check samples exist\n",
    "print(\"\\nSample sizes:\")\n",
    "print(\"RQ1/for-real-validation:\", len(sample_real))\n",
    "print(\"RQ3/for-bh-madefake-final-r4k:\", len(sample_fake_1))\n",
    "print(\"RQ3/for-rawnet-madefake-final-r4k:\", len(sample_fake_2))\n",
    "print(\"RQ3/for-deep4s-madefake-final-r4k:\", len(sample_fake_deep4s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample sizes:\n",
      "RQ1/for-real-validation  4500\n",
      "RQ3/for-bh-madefake-final-r4k  2995\n",
      "RQ3/for-rawnet-madefake-final-r4k  1505\n",
      "RQ3/for-deep4s-madefake-final-r4k  1000\n",
      "Total number of files: 10000\n",
      "Number of real files: 4500\n",
      "Number of fake files: 5500\n",
      "\n",
      "Labels for the first 3 files:\n",
      "Data\\DeepVC-Dataset\\RQ1\\for-real-validation\\file31830.wav_16k.wav_norm.wav_mono.wav_silence.wav -> real\n",
      "Data\\DeepVC-Dataset\\RQ1\\for-real-validation\\file30610.wav_16k.wav_norm.wav_mono.wav_silence.wav -> real\n",
      "Data\\DeepVC-Dataset\\RQ1\\for-real-validation\\file239.wav_16k.wav_norm.wav_mono.wav_silence.wav -> real\n",
      "\n",
      "Labels for the last 3 files:\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file32831.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file6050.wav_16k.wav_norm.wav_mono.wav_silence.wav_00.wav.noisered.wav -> fake\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file13604.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n",
      "\n",
      "Total number of files in data_deep4s: 1000\n",
      "\n",
      "Labels for the first 3 files in data_deep4s:\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file6893.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file4063.wav_16k.wav_norm.wav_mono.wav_silence.wav_01.wav.noisered.wav -> fake\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file12191.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n",
      "\n",
      "Labels for the last 3 files in data_deep4s:\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file32831.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file6050.wav_16k.wav_norm.wav_mono.wav_silence.wav_00.wav.noisered.wav -> fake\n",
      "Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k\\file13604.wav_16k.wav_norm.wav_mono.wav_silence.wav_02.wav.noisered.wav -> fake\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Define directories for the datasets\n",
    "dir_real_for = r'Data\\DeepVC-Dataset\\RQ1\\for-real-validation'\n",
    "dir_fake_farid = r'Data\\DeepVC-Dataset\\RQ3\\for-bh-madefake-final-r4k'\n",
    "dir_fake_rawnet = r'Data\\DeepVC-Dataset\\RQ3\\for-rawnet-madefake-final-r4k'\n",
    "dir_fake_deep4s = r'Data\\DeepVC-Dataset\\RQ3\\for-deep4s-madefake-final-r4k'\n",
    "\n",
    "# List all .wav files in each directory\n",
    "files_real_for = [os.path.join(dir_real_for, f) for f in os.listdir(dir_real_for) if f.endswith('.wav')]\n",
    "files_fake_farid = [os.path.join(dir_fake_farid, f) for f in os.listdir(dir_fake_farid) if f.endswith('.wav')]\n",
    "files_fake_rawnet = [os.path.join(dir_fake_rawnet, f) for f in os.listdir(dir_fake_rawnet) if f.endswith('.wav')]\n",
    "files_fake_deep4s = [os.path.join(dir_fake_deep4s, f) for f in os.listdir(dir_fake_deep4s) if f.endswith('.wav')]\n",
    "\n",
    "# Sample files from each dataset\n",
    "sample_real = random.sample(files_real_for, 4500)\n",
    "sample_fake_1 = random.sample(files_fake_farid, 2995)\n",
    "sample_fake_2 = random.sample(files_fake_rawnet, 1505)\n",
    "sample_fake_deep4s = random.sample(files_fake_deep4s, 1000)\n",
    "\n",
    "# Print the sizes of the sampled datasets\n",
    "print(\"\\nSample sizes:\")\n",
    "print(\"RQ1/for-real-validation \", len(sample_real))\n",
    "print(\"RQ3/for-bh-madefake-final-r4k \", len(sample_fake_1))\n",
    "print(\"RQ3/for-rawnet-madefake-final-r4k \", len(sample_fake_2))\n",
    "print(\"RQ3/for-deep4s-madefake-final-r4k \", len(sample_fake_deep4s))\n",
    "\n",
    "# Create a dictionary to store labels\n",
    "data = {}\n",
    "for file_path in sample_real:\n",
    "    data[file_path] = 'real'\n",
    "for file_path in sample_fake_1 + sample_fake_2 + sample_fake_deep4s:\n",
    "    data[file_path] = 'fake'\n",
    "\n",
    "# Count the number of real and fake files\n",
    "num_real = sum(1 for label in data.values() if label == 'real')\n",
    "num_fake = sum(1 for label in data.values() if label == 'fake')\n",
    "\n",
    "# Print total number of files and the counts of real and fake\n",
    "print(\"Total number of files:\", len(data))\n",
    "print(\"Number of real files:\", num_real)\n",
    "print(\"Number of fake files:\", num_fake)\n",
    "\n",
    "# Display labels for the first 3 files\n",
    "print(\"\\nLabels for the first 3 files:\")\n",
    "for file_path, label in list(data.items())[:3]:\n",
    "    print(file_path, \"->\", label)\n",
    "\n",
    "if len(data) >= 3:\n",
    "    print(\"\\nLabels for the last 3 files:\")\n",
    "    for file_path, label in list(data.items())[-3:]:\n",
    "        print(file_path, \"->\", label)\n",
    "else:\n",
    "    print(\"\\nNot enough files to display the last 3.\")\n",
    "\n",
    "# Create a separate dictionary for data_deep4s\n",
    "data_deep4s = {file_path: 'fake' for file_path in sample_fake_deep4s}\n",
    "\n",
    "# Print the total number of files in data_deep4s\n",
    "print(\"\\nTotal number of files in data_deep4s:\", len(data_deep4s))\n",
    "\n",
    "# Display labels for the first 3 files in data_deep4s\n",
    "print(\"\\nLabels for the first 3 files in data_deep4s:\")\n",
    "for file_path, label in list(data_deep4s.items())[:3]:\n",
    "    print(file_path, \"->\", label)\n",
    "\n",
    "if len(data_deep4s) >= 3:\n",
    "    print(\"\\nLabels for the last 3 files in data_deep4s:\")\n",
    "    for file_path, label in list(data_deep4s.items())[-3:]:\n",
    "        print(file_path, \"->\", label)\n",
    "else:\n",
    "    print(\"\\nNot enough files in data_deep4s to display the last 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "* Train: 70%\n",
    "* Validation: 15%\n",
    "* Test: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7000\n",
      "Validation set size: 1500\n",
      "Testing set size: 1500\n",
      "Deep4S set size: 1000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "# Shuffle the dictionary keys\n",
    "keys = list(data.keys())\n",
    "random.shuffle(keys)\n",
    "deep4s_keys = list(data_deep4s.keys())\n",
    "random.shuffle(deep4s_keys)\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "total_size = len(keys)\n",
    "train_size = int(total_size * 0.7)\n",
    "val_size = int(total_size * 0.15)\n",
    "\n",
    "# Divide the keys into training, validation, and testing sets\n",
    "train_keys = keys[:train_size]\n",
    "val_keys = keys[train_size:train_size + val_size]\n",
    "test_keys = keys[train_size + val_size:]\n",
    "\n",
    "# Retrieve the corresponding file paths and labels for each set\n",
    "train_set = [(key, data[key]) for key in train_keys]\n",
    "val_set = [(key, data[key]) for key in val_keys]\n",
    "test_set = [(key, data[key]) for key in test_keys]\n",
    "deep4s_set = [(key, data_deep4s[key]) for key in deep4s_keys]\n",
    "\n",
    "# Create the output directory\n",
    "output_dir = 'filtered_data'  # Directory for filtered data\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for train, val, and test sets\n",
    "os.makedirs(os.path.join(output_dir, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'test'), exist_ok=True)\n",
    "\n",
    "# Function to save file paths to their respective directories\n",
    "def save_to_directory(data_set, directory):\n",
    "    for file_path, label in data_set:\n",
    "        # Copy the file to the appropriate directory\n",
    "        shutil.copy(file_path, os.path.join(directory, os.path.basename(file_path)))\n",
    "\n",
    "# Save each set\n",
    "save_to_directory(train_set, os.path.join(output_dir, 'train'))\n",
    "save_to_directory(val_set, os.path.join(output_dir, 'val'))\n",
    "save_to_directory(test_set, os.path.join(output_dir, 'test'))\n",
    "\n",
    "# Print sizes of each set\n",
    "print(\"Training set size:\", len(train_set))\n",
    "print(\"Validation set size:\", len(val_set))\n",
    "print(\"Testing set size:\", len(test_set))\n",
    "print(\"Deep4S set size:\", len(deep4s_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ('Data\\\\DeepVC-Dataset\\\\RQ1\\\\for-real-validation\\\\file32394.wav_16k.wav_norm.wav_mono.wav_silence.wav', 'real')\n",
      "\n",
      " ('Data\\\\DeepVC-Dataset\\\\RQ3\\\\for-bh-madefake-final-r4k\\\\file14267.wav_16k.wav_norm.wav_mono.wav_silence.wav_00.wav.noisered.wav', 'fake')\n",
      "\n",
      " ('Data\\\\DeepVC-Dataset\\\\RQ3\\\\for-deep4s-madefake-final-r4k\\\\file1539.wav_16k.wav_norm.wav_mono.wav_silence.wav_01.wav.noisered.wav', 'fake')\n",
      "\n",
      " ('Data\\\\DeepVC-Dataset\\\\RQ3\\\\for-deep4s-madefake-final-r4k\\\\file26442.wav_16k.wav_norm.wav_mono.wav_silence.wav_03.wav.noisered.wav', 'fake')\n"
     ]
    }
   ],
   "source": [
    "# Check 1 example from each\n",
    "print(\"\\n\", train_set[0])\n",
    "print(\"\\n\", val_set[0])\n",
    "print(\"\\n\", test_set[0])\n",
    "print(\"\\n\", deep4s_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (ONLY RUN ONCE)\n",
    "- no need to run if you have histograms stored in set folders\n",
    "- extracts histograms from SiF-DeepVC, not H-Voice. H-Voice already comes with histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "### Regular Function\n",
    "- no limitations\n",
    "- imiate H-voice histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "def compute_histogram(file_path, dir, iter):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Calculate histogram of audio\n",
    "    hist, bins = np.histogram(audio, bins=256, range=(-1, 1)) # Ours: 2^8 | Original: 2^16 bins\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure()\n",
    "    plt.bar(bins[:-1], hist, width=(bins[1] - bins[0]), color='black')\n",
    "    #plt.title('Histogram of Audio')\n",
    "    #plt.xlabel('Amplitude')\n",
    "    #plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(dir, f'hist_{iter}.png'))\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    #print(hist.shape)\n",
    "    #print(hist.dtype)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Function - Limit Histograms under 4 kHz (WIP)\n",
    "- Create a more generalized model by training our model on both H-Voice and SiF-DeepVC data sets.\n",
    "- Limit the histograms to below 4000 Hz. Since the SiF-DeepVC's handcrafted SiFs were designed at above 4k Hz, we want to test the model's capabilities when ignoring the SiFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Function to plot spectrogram\n",
    "def plot_spectrogram(audio, sr, title):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    spectrogram = librosa.display.specshow(librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max), sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.io.wavfile\n",
    "\n",
    "def filter(audio_data, cutoff_frequency, sr):\n",
    "    # Define the filter\n",
    "    nyquist_frequency = sr / 2\n",
    "    cutoff_normalized = cutoff_frequency / nyquist_frequency\n",
    "    b, a = scipy.signal.butter(4, cutoff_normalized, btype='low')\n",
    "\n",
    "    # Apply the filter to each channel\n",
    "    filtered_audio = np.apply_along_axis(lambda x: scipy.signal.filtfilt(b, a, x), axis=0, arr=audio_data)\n",
    "\n",
    "    return filtered_audio\n",
    "\n",
    "# Load the original audio file\n",
    "#sampling_rate, audio_data = scipy.io.wavfile.read(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,    15,    63,\n",
       "         170,   215,   328,   269,   366,   346,   954,  1595,  3593,\n",
       "        6557, 16118, 18024,  6899,  3516,  1853,   938,  1025,  1397,\n",
       "         789,   505,   282,    71,    59,   111,   125,   143,    63,\n",
       "          46,    39,    41,    54,    51,    52,    78,    65,    44,\n",
       "          32,    38,    32,    30,    30,    27,    31,    41,   111,\n",
       "         257,   162,    38,   122,   147,   196,   176,   121,    88,\n",
       "          82,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "def compute_histogram_filtered(file_path, dir, iter):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=44100)\n",
    "    # Plot spectrogram of original audio\n",
    "    #plot_spectrogram(audio, 44100, title='Original Audio Spectrogram')\n",
    "\n",
    "\n",
    "    cutoff_frequency = 4000\n",
    "    filtered_audio = filter(audio, cutoff_frequency, sr=44100)\n",
    "    # Plot spectrogram of filtered audio\n",
    "    #plot_spectrogram(filtered_audio, 44100, title='Filtered Audio Spectrogram')\n",
    "    # Calculate histogram of audio\n",
    "    hist, bins = np.histogram(filtered_audio, bins=256, range=(-1, 1)) # 2^8 bins\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure()\n",
    "    plt.bar(bins[:-1], hist, width=(bins[1] - bins[0]), color='black')\n",
    "    #plt.title('Histogram of Audio')\n",
    "    #plt.xlabel('Amplitude')\n",
    "    #plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(dir, f'hist_{iter}.png'))\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    #print(hist.shape)\n",
    "    #print(hist.dtype)\n",
    "    return hist\n",
    "\n",
    "# Test\n",
    "compute_histogram_filtered(test_set[0][0], r\"E:\\Data_Voice-Id\\filtered_data\\train\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Histograms - Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = 'E:/Data_Voice-Id/'  # Update with your actual root directory\n",
    "\n",
    "# New save directory structure\n",
    "save_dir = os.path.join(root_dir, 'hist_filtered_data', 'training_data')\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "batch_size = 256\n",
    "num_batches = len(train_set) // batch_size  # Calculate the number of batches\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = (batch_index + 1) * batch_size\n",
    "\n",
    "    for i, (file_path, label) in enumerate(train_set[start_index:end_index]):\n",
    "        # Create a label directory within the new save directory\n",
    "        label_dir = os.path.join(save_dir, label)\n",
    "        if not os.path.isdir(label_dir):\n",
    "            os.makedirs(label_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Compute and save the histogram\n",
    "        hist = compute_histogram(file_path, label_dir, i + start_index)\n",
    "\n",
    "# Process the remaining items (if any) after the last full batch\n",
    "remaining_items = len(train_set) % batch_size\n",
    "if remaining_items > 0:\n",
    "    start_index = num_batches * batch_size\n",
    "    for i, (file_path, label) in enumerate(train_set[start_index:]):\n",
    "        label_dir = os.path.join(save_dir, label)\n",
    "        if not os.path.isdir(label_dir):\n",
    "            os.makedirs(label_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Compute and save the histogram\n",
    "        hist = compute_histogram(file_path, label_dir, i + start_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in : 5465\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory you want to count files in\n",
    "directory_path1 = r'hist_filtered_data\\training_data\\real'  # Use raw string or double backslashes\n",
    "directory_path2 = r'hist_filtered_data\\training_data\\fake'  # Use raw string or double backslashes\n",
    "\n",
    "# Count the number of files\n",
    "num_files1 = len([f for f in os.listdir(directory_path1) if os.path.isfile(os.path.join(directory_path1, f))])\n",
    "num_files2 = len([f for f in os.listdir(directory_path2) if os.path.isfile(os.path.join(directory_path2, f))])\n",
    "num_files=num_files1+num_files2\n",
    "\n",
    "print(f\"Total number of files in : {num_files}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'E:/Data_Voice-Id/'  # Update with your actual root directory\n",
    "\n",
    "# New save directory for validation data\n",
    "save_dir = os.path.join(root_dir, 'hist_filtered_data', 'validation_data')\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for file_path, label in val_set:\n",
    "    label_dir = os.path.join(save_dir, label)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        os.makedirs(label_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Compute and save the histogram\n",
    "    hist = compute_histogram(file_path, label_dir, iter)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "root_dir = 'E:/Data_Voice-Id/'  # Update with your actual root directory\n",
    "\n",
    "# New save directory for test data\n",
    "save_dir = os.path.join(root_dir, 'hist_filtered_data', 'test_data')\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for file_path, label in test_set:\n",
    "    label_dir = os.path.join(save_dir, label)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        os.makedirs(label_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Compute and save the histogram\n",
    "    hist = compute_histogram(file_path, label_dir, iter)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "root_dir = 'E:/Data_Voice-Id/'  # Update with your actual root directory\n",
    "\n",
    "# New save directory for Deep4SNet target test data\n",
    "save_dir = os.path.join(root_dir, 'hist_filtered_data', 'deep4s_data')\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for file_path, label in deep4s_set:\n",
    "    label_dir = os.path.join(save_dir, label)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        os.makedirs(label_dir)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Compute and save the histogram\n",
    "    hist = compute_histogram(file_path, label_dir, iter)\n",
    "    iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count histograms in each directory\n",
    "def count_png_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "directories = [\n",
    "    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Training_Set/',\n",
    "    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Validation_Set/',\n",
    "    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Test_Set/',\n",
    "    root_dir + 'Voice_Cloning_Detection/Data/SiF-DeepVC/Deep4SNet_Target_Test_Set/'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    total_count = 0\n",
    "    print(\"Directory:\", directory)\n",
    "    for sub_dir in os.listdir(directory):\n",
    "        sub_dir_path = os.path.join(directory, sub_dir)\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            png_count = count_png_files(sub_dir_path)\n",
    "            total_count += png_count\n",
    "            print(\"   Subdirectory:\", sub_dir, \"| Histograms:\", png_count)\n",
    "    print(\"Total histograms: \", total_count)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
